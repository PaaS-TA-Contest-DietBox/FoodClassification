{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Conv2D, Flatten\n",
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f'../Datasets/kfood/'\n",
    "CKPT_PATH = f'./checkpoints/metric_learning_model.h5'\n",
    "LOG_PATH = './logs/metric_model/' + datetime.now().strftime(\"%Y%n%d-%H%M%S\")\n",
    "\n",
    "size = 224\n",
    "batch_size = 512\n",
    "\n",
    "classes = 150\n",
    "\n",
    "learning_rate = 1e-2\n",
    "wd = 0.0005\n",
    "max_lr = 1e-2\n",
    "min_lr = 5e-5\n",
    "cycle_len = 20\n",
    "\n",
    "EPOCHS = 100\n",
    "use_pretrained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150513 images belonging to 150 classes.\n"
     ]
    }
   ],
   "source": [
    "# featurewise_center=True,\n",
    "# featurewise_std_normalization=True,\n",
    "# rotation_range=20,\n",
    "# width_shift_range=0.2,\n",
    "# height_shift_range=0.2,\n",
    "# horizontal_flip=True,\n",
    "# vertical_flip=True,\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    ")\n",
    "\n",
    "datagenerator = datagen.flow_from_directory(\n",
    "    DATA_PATH,\n",
    "    target_size=(size, size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(\n",
    "        input_shape=(size, size, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "    )\n",
    "base_model.trainable = False\n",
    "\n",
    "def MetricLearning(base_model):\n",
    "    x = inputs = Input([size, size, 3])\n",
    "    x = base_model(x)\n",
    "    x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='elu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512, activation='elu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(256, activation=None)(x)\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model = MetricLearning(base_model)\n",
    "    if use_pretrained:\n",
    "        model.load_weights(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 7, 7, 32)          368672    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               803328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "=================================================================\n",
      "Total params: 3,823,968\n",
      "Trainable params: 1,565,984\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tfa.losses.TripletSemiHardLoss(\n",
    "        distance_metric='angular'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=CKPT_PATH,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def trainfle_fn(x):\n",
    "    return 1. / (2.**(x - 1))\n",
    "clr_f = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=(max_lr+min_lr)/2,\n",
    "    maximal_learning_rate=max_lr,\n",
    "    step_size=cycle_len,\n",
    "    scale_fn=trainfle_fn\n",
    ")\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(clr_f)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0,\n",
    "    patience=30,\n",
    "    verbose=2,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=LOG_PATH,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "callbacks = [ckpt, lr_scheduler, early_stop, tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/294 [..............................] - ETA: 0s - loss: 0.9870WARNING:tensorflow:From /home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  4/294 [..............................] - ETA: 5:55 - loss: 0.9875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 1498 bytes but only got 0. Skipping tag 37500\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 125 bytes but only got 120. Skipping tag 37510\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 41988\n",
      "  \"Possibly corrupt EXIF data.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  5/294 [..............................] - ETA: 6:31 - loss: 0.9878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/Image.py:952: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24/294 [=>............................] - ETA: 8:36 - loss: 0.9888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:792: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 4. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69/294 [======>.......................] - ETA: 7:29 - loss: 0.9887"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 1078 bytes but only got 0. Skipping tag 37500\n",
      "  \"Possibly corrupt EXIF data.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/294 [===============>..............] - ETA: 4:40 - loss: 0.9887"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 2. Skipping tag 41486\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 41487\n",
      "  \"Possibly corrupt EXIF data.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/294 [================>.............] - ETA: 4:05 - loss: 0.9887"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 2. Skipping tag 37386\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 37396\n",
      "  \"Possibly corrupt EXIF data.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/294 [=======================>......] - ETA: 1:43 - loss: 0.9886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungjin/anaconda3/envs/Pasta/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 6. Skipping tag 272\n",
      "  \"Possibly corrupt EXIF data.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - ETA: 0s - loss: 0.9886\n",
      "Epoch 00001: loss improved from inf to 0.98860, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 607s 2s/step - loss: 0.9886\n",
      "Epoch 2/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9886\n",
      "Epoch 00002: loss improved from 0.98860 to 0.98859, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 598s 2s/step - loss: 0.9886\n",
      "Epoch 3/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9886\n",
      "Epoch 00003: loss improved from 0.98859 to 0.98856, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 595s 2s/step - loss: 0.9886\n",
      "Epoch 4/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9885\n",
      "Epoch 00004: loss improved from 0.98856 to 0.98850, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 596s 2s/step - loss: 0.9885\n",
      "Epoch 5/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9884\n",
      "Epoch 00005: loss improved from 0.98850 to 0.98844, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 595s 2s/step - loss: 0.9884\n",
      "Epoch 6/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9883\n",
      "Epoch 00006: loss improved from 0.98844 to 0.98833, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 595s 2s/step - loss: 0.9883\n",
      "Epoch 7/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9884\n",
      "Epoch 00007: loss did not improve from 0.98833\n",
      "294/294 [==============================] - 595s 2s/step - loss: 0.9884\n",
      "Epoch 8/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9884\n",
      "Epoch 00008: loss did not improve from 0.98833\n",
      "294/294 [==============================] - 595s 2s/step - loss: 0.9884\n",
      "Epoch 9/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9883\n",
      "Epoch 00009: loss improved from 0.98833 to 0.98831, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 596s 2s/step - loss: 0.9883\n",
      "Epoch 10/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9881\n",
      "Epoch 00010: loss improved from 0.98831 to 0.98808, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 596s 2s/step - loss: 0.9881\n",
      "Epoch 11/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9880\n",
      "Epoch 00011: loss improved from 0.98808 to 0.98803, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9880\n",
      "Epoch 12/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9880\n",
      "Epoch 00012: loss improved from 0.98803 to 0.98802, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 598s 2s/step - loss: 0.9880\n",
      "Epoch 13/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9879\n",
      "Epoch 00013: loss improved from 0.98802 to 0.98795, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9879\n",
      "Epoch 14/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9880\n",
      "Epoch 00014: loss did not improve from 0.98795\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9880\n",
      "Epoch 15/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9878\n",
      "Epoch 00015: loss improved from 0.98795 to 0.98784, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9878\n",
      "Epoch 16/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9878\n",
      "Epoch 00016: loss improved from 0.98784 to 0.98776, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9878\n",
      "Epoch 17/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9877\n",
      "Epoch 00017: loss improved from 0.98776 to 0.98771, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 596s 2s/step - loss: 0.9877\n",
      "Epoch 18/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9877\n",
      "Epoch 00018: loss improved from 0.98771 to 0.98766, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 605s 2s/step - loss: 0.9877\n",
      "Epoch 19/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9875\n",
      "Epoch 00019: loss improved from 0.98766 to 0.98755, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 604s 2s/step - loss: 0.9875\n",
      "Epoch 20/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9874\n",
      "Epoch 00020: loss improved from 0.98755 to 0.98744, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 598s 2s/step - loss: 0.9874\n",
      "Epoch 21/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9874\n",
      "Epoch 00021: loss improved from 0.98744 to 0.98743, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9874\n",
      "Epoch 22/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9872\n",
      "Epoch 00022: loss improved from 0.98743 to 0.98722, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9872\n",
      "Epoch 23/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9873\n",
      "Epoch 00023: loss did not improve from 0.98722\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9873\n",
      "Epoch 24/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9873\n",
      "Epoch 00024: loss did not improve from 0.98722\n",
      "294/294 [==============================] - 598s 2s/step - loss: 0.9873\n",
      "Epoch 25/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9872\n",
      "Epoch 00025: loss did not improve from 0.98722\n",
      "294/294 [==============================] - 602s 2s/step - loss: 0.9872\n",
      "Epoch 26/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9870\n",
      "Epoch 00026: loss improved from 0.98722 to 0.98699, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9870\n",
      "Epoch 27/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9870\n",
      "Epoch 00027: loss improved from 0.98699 to 0.98697, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9870\n",
      "Epoch 28/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9869\n",
      "Epoch 00028: loss improved from 0.98697 to 0.98686, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 596s 2s/step - loss: 0.9869\n",
      "Epoch 29/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9869\n",
      "Epoch 00029: loss did not improve from 0.98686\n",
      "294/294 [==============================] - 597s 2s/step - loss: 0.9869\n",
      "Epoch 30/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9867\n",
      "Epoch 00030: loss improved from 0.98686 to 0.98672, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 599s 2s/step - loss: 0.9867\n",
      "Epoch 31/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9867\n",
      "Epoch 00031: loss improved from 0.98672 to 0.98672, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 595s 2s/step - loss: 0.9867\n",
      "Epoch 32/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9867\n",
      "Epoch 00032: loss improved from 0.98672 to 0.98668, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 586s 2s/step - loss: 0.9867\n",
      "Epoch 33/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9867\n",
      "Epoch 00033: loss improved from 0.98668 to 0.98666, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9867\n",
      "Epoch 34/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9864\n",
      "Epoch 00034: loss improved from 0.98666 to 0.98644, saving model to ./checkpoints/metric_learning_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 585s 2s/step - loss: 0.9864\n",
      "Epoch 35/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9866\n",
      "Epoch 00035: loss did not improve from 0.98644\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9866\n",
      "Epoch 36/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9865\n",
      "Epoch 00036: loss did not improve from 0.98644\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9865\n",
      "Epoch 37/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9864\n",
      "Epoch 00037: loss improved from 0.98644 to 0.98637, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9864\n",
      "Epoch 38/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9864\n",
      "Epoch 00038: loss improved from 0.98637 to 0.98637, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9864\n",
      "Epoch 39/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9864\n",
      "Epoch 00039: loss did not improve from 0.98637\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9864\n",
      "Epoch 40/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9864\n",
      "Epoch 00040: loss improved from 0.98637 to 0.98636, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 586s 2s/step - loss: 0.9864\n",
      "Epoch 41/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9863\n",
      "Epoch 00041: loss improved from 0.98636 to 0.98630, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 586s 2s/step - loss: 0.9863\n",
      "Epoch 42/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9863\n",
      "Epoch 00042: loss improved from 0.98630 to 0.98629, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9863\n",
      "Epoch 43/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9863\n",
      "Epoch 00043: loss did not improve from 0.98629\n",
      "294/294 [==============================] - 586s 2s/step - loss: 0.9863\n",
      "Epoch 44/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9862\n",
      "Epoch 00044: loss improved from 0.98629 to 0.98615, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 586s 2s/step - loss: 0.9862\n",
      "Epoch 45/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9862\n",
      "Epoch 00045: loss did not improve from 0.98615\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9862\n",
      "Epoch 46/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9862\n",
      "Epoch 00046: loss did not improve from 0.98615\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9862\n",
      "Epoch 47/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9861\n",
      "Epoch 00047: loss improved from 0.98615 to 0.98615, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9861\n",
      "Epoch 48/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9860\n",
      "Epoch 00048: loss improved from 0.98615 to 0.98605, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9860\n",
      "Epoch 49/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9860\n",
      "Epoch 00049: loss improved from 0.98605 to 0.98600, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9860\n",
      "Epoch 50/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9860\n",
      "Epoch 00050: loss did not improve from 0.98600\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9860\n",
      "Epoch 51/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9859\n",
      "Epoch 00051: loss improved from 0.98600 to 0.98593, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9859\n",
      "Epoch 52/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9859\n",
      "Epoch 00052: loss improved from 0.98593 to 0.98585, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9859\n",
      "Epoch 53/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9859\n",
      "Epoch 00053: loss did not improve from 0.98585\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9859\n",
      "Epoch 54/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9859\n",
      "Epoch 00054: loss did not improve from 0.98585\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9859\n",
      "Epoch 55/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9858\n",
      "Epoch 00055: loss improved from 0.98585 to 0.98579, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9858\n",
      "Epoch 56/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9858\n",
      "Epoch 00056: loss improved from 0.98579 to 0.98577, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9858\n",
      "Epoch 57/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9858\n",
      "Epoch 00057: loss improved from 0.98577 to 0.98576, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9858\n",
      "Epoch 58/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9856\n",
      "Epoch 00058: loss improved from 0.98576 to 0.98555, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9856\n",
      "Epoch 59/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9856\n",
      "Epoch 00059: loss did not improve from 0.98555\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9856\n",
      "Epoch 60/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9855\n",
      "Epoch 00060: loss improved from 0.98555 to 0.98551, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9855\n",
      "Epoch 61/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9855\n",
      "Epoch 00061: loss improved from 0.98551 to 0.98546, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9855\n",
      "Epoch 62/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9855\n",
      "Epoch 00062: loss did not improve from 0.98546\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9855\n",
      "Epoch 63/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9854\n",
      "Epoch 00063: loss improved from 0.98546 to 0.98541, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9854\n",
      "Epoch 64/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9853\n",
      "Epoch 00064: loss improved from 0.98541 to 0.98531, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9853\n",
      "Epoch 65/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9853\n",
      "Epoch 00065: loss improved from 0.98531 to 0.98530, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 585s 2s/step - loss: 0.9853\n",
      "Epoch 66/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9852\n",
      "Epoch 00066: loss improved from 0.98530 to 0.98520, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9852\n",
      "Epoch 67/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9853\n",
      "Epoch 00067: loss did not improve from 0.98520\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9853\n",
      "Epoch 68/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9852\n",
      "Epoch 00068: loss improved from 0.98520 to 0.98516, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9852\n",
      "Epoch 69/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9853\n",
      "Epoch 00069: loss did not improve from 0.98516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 584s 2s/step - loss: 0.9853\n",
      "Epoch 70/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9852\n",
      "Epoch 00070: loss improved from 0.98516 to 0.98515, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9852\n",
      "Epoch 71/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9850\n",
      "Epoch 00071: loss improved from 0.98515 to 0.98503, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9850\n",
      "Epoch 72/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9850\n",
      "Epoch 00072: loss improved from 0.98503 to 0.98499, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9850\n",
      "Epoch 73/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9850\n",
      "Epoch 00073: loss did not improve from 0.98499\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9850\n",
      "Epoch 74/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9850\n",
      "Epoch 00074: loss did not improve from 0.98499\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9850\n",
      "Epoch 75/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00075: loss improved from 0.98499 to 0.98480, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9848\n",
      "Epoch 76/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00076: loss did not improve from 0.98480\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9848\n",
      "Epoch 77/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00077: loss improved from 0.98480 to 0.98479, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9848\n",
      "Epoch 78/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9847\n",
      "Epoch 00078: loss improved from 0.98479 to 0.98470, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9847\n",
      "Epoch 79/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00079: loss did not improve from 0.98470\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9848\n",
      "Epoch 80/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00080: loss did not improve from 0.98470\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9848\n",
      "Epoch 81/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9846\n",
      "Epoch 00081: loss improved from 0.98470 to 0.98457, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9846\n",
      "Epoch 82/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9848\n",
      "Epoch 00082: loss did not improve from 0.98457\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9848\n",
      "Epoch 83/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9846\n",
      "Epoch 00083: loss did not improve from 0.98457\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9846\n",
      "Epoch 84/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9847\n",
      "Epoch 00084: loss did not improve from 0.98457\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9847\n",
      "Epoch 85/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9846\n",
      "Epoch 00085: loss did not improve from 0.98457\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9846\n",
      "Epoch 86/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9846\n",
      "Epoch 00086: loss improved from 0.98457 to 0.98457, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9846\n",
      "Epoch 87/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9845\n",
      "Epoch 00087: loss improved from 0.98457 to 0.98447, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9845\n",
      "Epoch 88/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9844\n",
      "Epoch 00088: loss improved from 0.98447 to 0.98440, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9844\n",
      "Epoch 89/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9845\n",
      "Epoch 00089: loss did not improve from 0.98440\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9845\n",
      "Epoch 90/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9842\n",
      "Epoch 00090: loss improved from 0.98440 to 0.98423, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9842\n",
      "Epoch 91/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9844\n",
      "Epoch 00091: loss did not improve from 0.98423\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9844\n",
      "Epoch 92/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00092: loss did not improve from 0.98423\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9843\n",
      "Epoch 93/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00093: loss did not improve from 0.98423\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9843\n",
      "Epoch 94/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00094: loss did not improve from 0.98423\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9843\n",
      "Epoch 95/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9842\n",
      "Epoch 00095: loss did not improve from 0.98423\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9842\n",
      "Epoch 96/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9842\n",
      "Epoch 00096: loss improved from 0.98423 to 0.98417, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9842\n",
      "Epoch 97/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9843\n",
      "Epoch 00097: loss did not improve from 0.98417\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9843\n",
      "Epoch 98/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9841\n",
      "Epoch 00098: loss improved from 0.98417 to 0.98414, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9841\n",
      "Epoch 99/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9841\n",
      "Epoch 00099: loss improved from 0.98414 to 0.98410, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 583s 2s/step - loss: 0.9841\n",
      "Epoch 100/100\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9840\n",
      "Epoch 00100: loss improved from 0.98410 to 0.98402, saving model to ./checkpoints/metric_learning_model.h5\n",
      "294/294 [==============================] - 584s 2s/step - loss: 0.9840\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagenerator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_model = MetricLearning(base_model)\n",
    "metric_model.load_weights(CKPT_PATH)\n",
    "metric_model.trainable = False\n",
    "\n",
    "def Classification(base_model):\n",
    "    x = inputs = Input([size, size, 3])\n",
    "    x = base_model(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(classes, activation='softmax')(x)\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    c_model = Classification(metric_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "functional_15 (Functional)   (None, 256)               3823968   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 150)               38550     \n",
      "=================================================================\n",
      "Total params: 3,928,310\n",
      "Trainable params: 104,342\n",
      "Non-trainable params: 3,823,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "c_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate, wd),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_CKPT_PATH = f'./checkpoints/classification_model.h5'\n",
    "C_LOG_PATH = './logs/classification_model/' + datetime.now().strftime(\"%Y%n%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=C_CKPT_PATH,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def trainfle_fn(x):\n",
    "    return 1. / (2.**(x - 1))\n",
    "clr_f = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=(max_lr+min_lr)/2,\n",
    "    maximal_learning_rate=max_lr,\n",
    "    step_size=cycle_len,\n",
    "    scale_fn=trainfle_fn\n",
    ")\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(clr_f)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=30,\n",
    "    verbose=2,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=LOG_PATH,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "callbacks = [ckpt, lr_scheduler, early_stop, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120448 images belonging to 150 classes.\n",
      "Found 30065 images belonging to 150 classes.\n"
     ]
    }
   ],
   "source": [
    "c_datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "c_train_datagenerator = c_datagen.flow_from_directory(\n",
    "    DATA_PATH,\n",
    "    target_size=(size, size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    subset='training',\n",
    ")\n",
    "\n",
    "c_val_datagenerator = c_datagen.flow_from_directory(\n",
    "    DATA_PATH,\n",
    "    target_size=(size, size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    subset='validation',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 2.1121 - accuracy: 0.4223\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.45232, saving model to ./checkpoints/classification_model.h5\n",
      "236/236 [==============================] - 588s 2s/step - loss: 2.1121 - accuracy: 0.4223 - val_loss: 1.9516 - val_accuracy: 0.4523\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 2.0857 - accuracy: 0.4252\n",
      "Epoch 00002: val_accuracy improved from 0.45232 to 0.46097, saving model to ./checkpoints/classification_model.h5\n",
      "236/236 [==============================] - 587s 2s/step - loss: 2.0857 - accuracy: 0.4252 - val_loss: 1.9412 - val_accuracy: 0.4610\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 2.1017 - accuracy: 0.4209\n",
      "Epoch 00003: val_accuracy did not improve from 0.46097\n",
      "236/236 [==============================] - 590s 2s/step - loss: 2.1017 - accuracy: 0.4209 - val_loss: 1.9806 - val_accuracy: 0.4496\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 2.1055 - accuracy: 0.4197\n",
      "Epoch 00004: val_accuracy did not improve from 0.46097\n",
      "236/236 [==============================] - 590s 3s/step - loss: 2.1055 - accuracy: 0.4197 - val_loss: 1.9710 - val_accuracy: 0.4540\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 2.1159 - accuracy: 0.4190\n",
      "Epoch 00005: val_accuracy did not improve from 0.46097\n",
      "236/236 [==============================] - 593s 3s/step - loss: 2.1159 - accuracy: 0.4190 - val_loss: 1.9920 - val_accuracy: 0.4423\n",
      "Epoch 6/100\n",
      "101/236 [===========>..................] - ETA: 4:10 - loss: 2.1041 - accuracy: 0.4215"
     ]
    }
   ],
   "source": [
    "history = c_model.fit(\n",
    "    c_train_datagenerator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=c_val_datagenerator,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pasta",
   "language": "python",
   "name": "pasta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
